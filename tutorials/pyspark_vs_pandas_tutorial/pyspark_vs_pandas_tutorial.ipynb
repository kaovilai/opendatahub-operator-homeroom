{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Wrangling comparison between PySpark and Pandas \n",
    "Note: in Spark 2.4 we will also have the Koalas API for spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import a few libraries\n",
    "\n",
    "!pip install wget\n",
    "import wget\n",
    "import sys\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENTER VARIABLE VALUES FOR THE TUTORIAL HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['S3_ENDPOINT'] = 'ENTER LOCATION OF CEPH HERE' #ex. 'https://my.ceph.cluster'\n",
    "ceph_bucket = 'ENTER CEPH BUCKET HERE' #ex. 'TUTORIAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Create a S3 client that will access Ceph\n",
    "s3 = boto3.client('s3','us-east-1', endpoint_url= os.environ['S3_ENDPOINT'],\n",
    "                       aws_access_key_id = os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                       aws_secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "\n",
    "ceph_bucket = ceph_bucket.upper()\n",
    "\n",
    "#Define the location to upload data to Ceph\n",
    "s3.create_bucket(Bucket=ceph_bucket)\n",
    "\n",
    "ceph_base_location = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "#Upload a sample object and verify\n",
    "s3.put_object(Bucket=ceph_bucket,Key=ceph_base_location + '/my_created_object',Body='data')\n",
    "for key in s3.list_objects(Bucket=ceph_bucket)['Contents']:\n",
    "    print(key['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the data for the example by uploading it to Ceph Object Storage\n",
    "filename = wget.download(url=\"https://raw.githubusercontent.com/pandas-dev/pandas/master/pandas/tests/data/iris.csv\", out='iris.csv')\n",
    "s3.upload_file(filename, ceph_bucket, ceph_base_location + \"/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "spark = SparkSession.builder.appName(os.environ['JUPYTERHUB_USER'] + ' Iris Example').master('spark://' + os.environ['SPARK_CLUSTER'] + ':7077').getOrCreate()\n",
    "\n",
    "#Configure Spark to access data from Ceph\n",
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", os.environ['S3_ENDPOINT'])\n",
    "hadoopConf.set(\"fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "iris_spark_df = spark.read.csv(\"s3a://\" + ceph_bucket + \"/\" + ceph_base_location + \"/iris.csv\", header=\"true\", inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_spark_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run similar commands in Pandas to read into the dataframe and inspect its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install s3fs\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False, key=os.environ['AWS_ACCESS_KEY_ID'], secret=os.environ['AWS_SECRET_ACCESS_KEY'], client_kwargs={'endpoint_url': \n",
    "    os.environ['S3_ENDPOINT']})\n",
    "\n",
    "iris_pandas_df = pd.read_csv(fs.open(\"s3://\" + ceph_bucket + \"/\" + ceph_base_location + \"/iris.csv\"))\n",
    "iris_pandas_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pandas_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pandas_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pandas_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupBy in Pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_spark_df.groupBy('Name').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupBy in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pandas_df['Name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename a column in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_rename_iris_df = iris_spark_df.toDF(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\")\n",
    "col_rename_iris_df.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename a column in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pandas_df.columns = ['sepal_length','sepal_width','petal_length','petal_width','species']\n",
    "iris_pandas_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop a column in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col_iris_df = iris_spark_df.drop('petal_width')\n",
    "drop_col_iris_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop a column in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col_iris_df2 = iris_pandas_df.drop('petal_width', axis=1)\n",
    "drop_col_iris_df2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a filter to a dataframe in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = col_rename_iris_df[col_rename_iris_df.sepal_length < 5 ]\n",
    "spark_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a filter to a dataframe in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = iris_pandas_df[iris_pandas_df.sepal_length < 5]\n",
    "pandas_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import built-in functions in pyspark (note: pandas uses numpy to accomplish this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df_log = col_rename_iris_df.withColumn('log_sep_len', F.log(col_rename_iris_df.sepal_length))\n",
    "df_log.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
