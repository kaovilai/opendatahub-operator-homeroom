{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intent of this notebook is to provide examples of how data scientists can use object storage, and more specifically, Ceph object storage, much in the same way they are accoustomed to interacting with Amazon Simple Storage Service (S3). This is made possible because Ceph's object storage gateway offers excellent fidelity with the modalities of Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Boto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boto is an integrated interface to current and future infrastructural services offered by Amazon Web Services. Amoung the services it provides interfaces for is Amazon S3. For lightweight analysis of data using python tools like numpy or pandas, it is handy to interact with data stored in object storage using pure python. This is where Boto shines. The base-notebook from [radanalyticsio](https://radanalytics.io) doesn't include Boto, but you can install it from the comfort of a notebook using the conda install command below. If you find yourself using Boto frequently, it might be worth modifying [base-notebook](https://github.com/radanalyticsio/base-notebook) and building a custom notebook image that includes Boto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --yes --quiet --prefix {sys.prefix} boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3','us-east-1', endpoint_url= os.environ['RGW_API_ENDPOINT'],\n",
    "                       aws_access_key_id = os.environ['RGW_USER_USER_KEY'],\n",
    "                       aws_secret_access_key = os.environ['RGW_USER_SECRET_KEY'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bucket, uploading and object (put), and listing the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3.create_bucket(Bucket='ceph-bucket')\n",
    "s3.put_object(Bucket='ceph-bucket',Key='object',Body='data')\n",
    "for key in s3.list_objects(Bucket='ceph-bucket')['Contents']:\n",
    "    print(key['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running an application you can either establish a Spark session locally in the notebook pod, or point it to a remote Spark cluster. Oshinko is a collection of components from the radanalyticsio community that aid in the provisioning and scaling of Spark clusters for intelligent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[3]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", os.environ['RGW_API_ENDPOINT'])\n",
    "hadoopConf.set(\"fs.s3a.access.key\", os.environ['RGW_USER_USER_KEY'])\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", os.environ['RGW_USER_SECRET_KEY'])\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "spark.range(100, numPartitions=100).rdd.map(lambda x: socket.gethostname()).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df0 = spark.read.text(\"s3a://ceph-bucket/object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a Hybrid Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of Hadoop 2.8, S3A supports per bucket configuration. This is very powerful. It allows us to have a distinct S3A configuration, with a different endpoint and different set of credentials. With this I can use a single Spark context to read a parquet file from a bucket in the public cloud (Amazon S3) into a data frame, then turn around and write that dataframe as a parquet file into a bucket that exists in the Ceph Nano service running in Minishift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Public to Private ETL__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply read tab separated data from a bucket in Amazon S3 and write it back out to a bucket in our Ceph Nano service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.read.csv(\"s3a://bd-dist/trip_report.tsv\",sep=\"\\t\").write.csv(\"s3a://ceph-bucket/trip_report.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all JSON files from a bucket prefix (pseudo directory) in Amazon S3 and write them back out to a bucket in our Ceph nano service with the same bucket prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.read.option(\"multiline\", True).option(\"mode\", \"PERMISSIVE\").json(\"s3a://bd-dist/kube-metrics\").repartition(76).write.option(\"compression\", \"bzip2\").json(\"s3a://ceph-bucket/kube-metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Prometheus data set from Ceph Nano into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonFile = spark.read.json(\"s3a://ceph-bucket/kube-metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import statistics libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Display schema of files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Display schema:')\n",
    "jsonFile.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Query the JSON data using filters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the created SchemaRDD as a temporary table.\n",
    "jsonFile.registerTempTable(\"kubelet_docker_operations_latency_microseconds\")\n",
    "\n",
    "#Filter the results into a data frame\n",
    "data = spark.sql(\"SELECT values, metric.operation_type FROM kubelet_docker_operations_latency_microseconds WHERE metric.quantile='0.9' AND metric.hostname='free-stg-master-03fb6'\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_pd = data.toPandas()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "OP_TYPE = 'list_images'\n",
    "\n",
    "df2 = pd.DataFrame(columns = ['utc_timestamp','value', 'operation_type'])\n",
    "#df2 ='\n",
    "for op in set(data_pd['operation_type']):\n",
    "    dict_raw = data_pd[data_pd['operation_type'] == op]['values']\n",
    "    list_raw = []\n",
    "    for key in dict_raw.keys():\n",
    "        list_raw.extend(dict_raw[key])\n",
    "    temp_frame = pd.DataFrame(list_raw, columns = ['utc_timestamp','value'])\n",
    "    temp_frame['operation_type'] = op\n",
    "    \n",
    "    df2 = df2.append(temp_frame)\n",
    "\n",
    "\n",
    "df2 = df2[df2['value'] != 'NaN']\n",
    "\n",
    "df2['value'] = df2['value'].apply(lambda a: int(a))\n",
    "\n",
    "df2['timestamp'] = df2['utc_timestamp'].apply(lambda a : datetime.fromtimestamp(int(a)))\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Objective - Verify Above Alerts __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store timestamp with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index(inplace =True)\n",
    "\n",
    "del df2['index']\n",
    "\n",
    "df2['operation_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segregate the values by operation type in separate variables as Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filtered_op_frame(op_type):\n",
    "    temp = df2[df2.operation_type == op_type]\n",
    "    temp = temp.sort_values(by='timestamp')\n",
    "    return temp\n",
    "\n",
    "operation_type_value = {}\n",
    "for temp in list(df2.operation_type.unique()):\n",
    "    operation_type_value[temp] = get_filtered_op_frame(temp)['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Descriptive Stats__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It refers to the portion of statistics dedicated to summarizing a total population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Mean_\n",
    "\n",
    "Arithmetic average of a range of values or quantities, computed by dividing the total of all values by the number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in operation_type_value.keys():\n",
    "    print(\"Mean of: \",temp, \" - \", np.mean(operation_type_value[temp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Variance_\n",
    "\n",
    "In the same way that the mean is used to describe the central tendency, variance is intended to describe the spread. The xi – μ is called the “deviation from the mean”, making the variance the squared deviation multiplied by 1 over the number of samples. This is why the square root of the variance, σ, is called the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in operation_type_value.keys():\n",
    "    print(\"Variance of: \",temp, \" - \", np.var(operation_type_value[temp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Standard Deviation_\n",
    "\n",
    "Standard deviation (SD, also represented by the Greek letter sigma σ or the Latin letter s) is a measure that is used to quantify the amount of variation or dispersion of a set of data values.[1] A low standard deviation indicates that the data points tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in operation_type_value.keys():\n",
    "    print(\"Standard Deviation of: \",temp, \" - \", np.std(operation_type_value[temp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Median_\n",
    "\n",
    "Denotes value or quantity lying at the midpoint of a frequency distribution of observed values or quantities, such that there is an equal probability of falling above or below it. Simply put, it is the middle value in the list of numbers. The median is a better choice when the indicator can be affected by some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in operation_type_value.keys():\n",
    "    print(\"Median of: \",temp, \" - \", np.median(operation_type_value[temp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Histogram__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common representation of a distribution is a histogram, which is a graph that shows the frequency or probability of each value. Plots will be generated by operation type\n",
    "\n",
    "We will use Seaborn module for this. __Kernel Density Estimation__ * will be added for smoothing.\n",
    "\n",
    "* In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample.\n",
    "* The kernel density estimate may be less familiar, but it can be a useful tool for plotting the shape of a distribution. Like the histogram, the KDE plots encodes the density of observations on one axis with height along the other axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(color_codes = True)\n",
    "\n",
    "for temp in operation_type_value.keys():\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,12))\n",
    "    sns.distplot(get_filtered_op_frame(temp)['value'], kde=True, ax=ax[0], axlabel= temp)\n",
    "    sns.distplot(np.log(get_filtered_op_frame(temp)['value']), kde=True, ax=ax[1], axlabel = \"Log transformed \"+ temp)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Understanding__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are all log normals, cause value will always be greater than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Box-Whisker__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.\n",
    "\n",
    "Log normalisation is required because, for different operations, values seems to be in very different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_whisker =  df2\n",
    "df_whisker['log_transformed_value'] = np.log(df2['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whisker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "ax = sns.boxplot(x=\"operation_type\", y=\"log_transformed_value\", hue=\"operation_type\", data=df_whisker)  # RUN PLOT   \n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding trend in time series, if there any__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trend means, if over time values have increasing or decreasing pattern. In this example we see that there is a trend of a slow and steady increase followed by a sharp drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "operation_type_value.keys()\n",
    "\n",
    "for temp in operation_type_value.keys():\n",
    "    #fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,12))\n",
    "    temp_frame = get_filtered_op_frame(temp)\n",
    "    temp_frame = temp_frame.set_index(temp_frame.timestamp)\n",
    "    temp_frame = temp_frame[['log_transformed_value']]\n",
    "    temp_frame.plot(figsize=(15,12),title=temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Local TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by installing TensorFlow, along with several other machine learning libraries that we will need for our machine learning example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --yes --quiet --prefix {sys.prefix} keras=2.1.2 scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RESTART NOTEBOOK KERNEL__\n",
    "\n",
    "Click on the \"__Kernel__\" dropdown in the menubar and select \"__Restart__\".\n",
    "\n",
    "This is to work around an issue where the Notebook Kernel is sometimes unable to import np_utils after Keras is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reinitiate Spark Context__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already established a Spark Context above, but we'll need reinitialize it now that the Notebook Kernel has been restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[3]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", os.environ['RGW_API_ENDPOINT'])\n",
    "hadoopConf.set(\"fs.s3a.access.key\", os.environ['RGW_USER_USER_KEY'])\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", os.environ['RGW_USER_SECRET_KEY'])\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Access the data using Spark__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feedbackFile = spark.read.csv(\"s3a://ceph-bucket/trip_report.tsv\",sep=\"\\t\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can load the trip report from the original sample TSV object in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feedbackFile = spark.read.csv(\"s3a://bd-dist/trip_report.tsv\",sep=\"\\t\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert the data to a Pandas data frame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = feedbackFile.toPandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Types of trip outcomes by field representative__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(sum(map(ord, \"categorical\")))\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "outcome_dict = {'Successful':0,'Partial Success':1,'Unsuccessful':2 }\n",
    "\n",
    "df_vis = df[['Your Name', 'Outcome']]\n",
    "df_vis['outcome_numeric'] = df_vis['Outcome'].apply(lambda a:outcome_dict[a])\n",
    "\n",
    "\n",
    "\n",
    "outcome_cross_table = pd.crosstab(index=df_vis[\"Your Name\"], \n",
    "                          columns=df_vis[\"Outcome\"])\n",
    "\n",
    "\n",
    "outcome_cross_table.plot(kind=\"bar\", \n",
    "                 figsize=(16,12),\n",
    "                 stacked=True,fontsize=12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Types of outcomes by event type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type_cross_table = pd.crosstab(index=df[\"Primary Audience Engaged\"], \n",
    "                          columns=df[\"Outcome\"])\n",
    "\n",
    "event_type_cross_table.plot(kind=\"bar\", \n",
    "                 figsize=(16,12),\n",
    "                 stacked=True,fontsize=12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now convert \"Highlights\" data to prepare for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Highlights'] = df['Highlights'].astype(str)\n",
    "\n",
    "df[['Highlights','Outcome']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcome = df[['Highlights','Outcome']]\n",
    "\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "grouped_highlights = pd.DataFrame(df_outcome.groupby('Outcome')['Highlights'].apply(lambda x: \"%s\" % ' '.join(x)))\n",
    "\n",
    "grouped_highlights['Outcome'] = list(grouped_highlights.index.get_values())\n",
    "grouped_highlights.reset_index(drop=True, inplace=True)\n",
    "\n",
    "grouped_highlights['Highlights'] = grouped_highlights['Highlights'].astype(str)\n",
    "\n",
    "df['Highlights'] = df['Highlights'].apply(lambda a: a.lower())\n",
    "\n",
    "df_success = df[df['Outcome'] == 'Successful']\n",
    "df_unsuccess = df[df['Outcome'] == 'Unsuccessful']\n",
    "df_part_success = df[df['Outcome'] == 'Partial Success']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import additional Machine Learning libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Separating train and test data. Taking successful and unsuccessful separately__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failure = df_part_success.append(df_unsuccess, ignore_index= True)\n",
    "\n",
    "df_failure['Outcome'] = 'Unsuccessful'\n",
    "\n",
    "test_hold_out = 0.1\n",
    "\n",
    "#### Success\n",
    "\n",
    "train = df_success[ : -int(test_hold_out * len(df_success))]\n",
    "test = df_success[-int(test_hold_out * len(df_success)) : ]\n",
    "\n",
    "#### Failure\n",
    "\n",
    "train = train.append(df_failure[ : -int(test_hold_out * len(df_failure))])\n",
    "test = test.append(df_failure[-int(test_hold_out * len(df_failure)) : ])\n",
    "\n",
    "\n",
    "train = train.sample(frac = 1)\n",
    "train['type'] = \"Train\"\n",
    "test['type'] = \"Test\"\n",
    "\n",
    "train = train.append(test)\n",
    "\n",
    "train.reset_index(drop=True,inplace=True)\n",
    "\n",
    "Y = pd.get_dummies(train['Outcome']).values\n",
    "\n",
    "test_index_list = list(train[train['type'] == 'Test'].index)\n",
    "\n",
    "test_index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the HIGHLIGHTS field for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__max_features__ = Vocabulary size, its a hyper parameter\n",
    "\n",
    "*Tokenizer creates vectors from text, mainly works like a dictionary id in total vocabulary, returns list of integers, where every integer acts like an index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_fatures = 10000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(train['Highlights'].values)\n",
    "X_highlights = tokenizer.texts_to_sequences(train['Highlights'].values)\n",
    "X_highlights = pad_sequences(X_highlights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating the network layer by layer__\n",
    "\n",
    "First layer is word embedding layer, second layer is LSTM based RNN, and third layer is Softmax activation layer, due to categorical outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X_highlights.shape[1], dropout=0.05))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.1, dropout_W=0.1))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Separating train and test data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_highlights_train = X_highlights[0:test_index_list[0]]\n",
    "Y_highlights_train = Y[0:test_index_list[0]]\n",
    "\n",
    "X_highlights_test = X_highlights[test_index_list[0]:]\n",
    "Y_highlights_test = Y[test_index_list[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Running the model__\n",
    "\n",
    "Batch size and number of epoch can be changed as optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "model.fit(X_highlights_train, Y_highlights_train, epochs = 10, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Printing test data accuracy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score,accuracy = model.evaluate(X_highlights_test, Y_highlights_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"accuracy: %.2f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model, tokenizer and feature dimension and store them in Ceph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"./model\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "feature_dimension = X_highlights_train.shape[1]\n",
    "with open('./feature_dimension.pickle', 'wb') as handle:\n",
    "    pickle.dump(feature_dimension, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#Create S3 session for writing manifest file\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = os.environ['RGW_USER_USER_KEY'],\n",
    "    aws_secret_access_key = os.environ['RGW_USER_SECRET_KEY']\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3', endpoint_url=os.environ['RGW_API_ENDPOINT'], verify=False)\n",
    "\n",
    "# Upload the model to S3\n",
    "s3.meta.client.upload_file('./model', 'ceph-bucket', 'models/trip_report_model')\n",
    "\n",
    "# Upload the tokenizer to S3\n",
    "s3.meta.client.upload_file('./tokenizer.pickle', 'ceph-bucket', 'models/trip_report_tokenizer.pickle')\n",
    "\n",
    "# Upload the feature dimension to S3\n",
    "s3.meta.client.upload_file('./feature_dimension.pickle', 'ceph-bucket', 'models/trip_report_feature_dimension.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been saved to s3 as binary objects and can be viewed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
