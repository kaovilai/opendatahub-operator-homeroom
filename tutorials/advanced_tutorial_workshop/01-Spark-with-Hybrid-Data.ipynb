{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intent of this notebook is to provide examples of how data scientists can use object storage, and more specifically, Ceph object storage, much in the same way they are accoustomed to interacting with Amazon Simple Storage Service (S3). This is made possible because Ceph's object storage gateway offers excellent fidelity with the modalities of Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Boto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boto is an integrated interface to current and future infrastructural services offered by Amazon Web Services. Amoung the services it provides interfaces for is Amazon S3. For lightweight analysis of data using python tools like numpy or pandas, it is handy to interact with data stored in object storage using pure python. This is where Boto shines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3','us-east-1', endpoint_url= os.environ['RGW_API_ENDPOINT'],\n",
    "                       aws_access_key_id = os.environ['ACCESSKEY'],\n",
    "                       aws_secret_access_key = os.environ['SECRETKEY'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bucket, uploading and object (put), and listing the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3.create_bucket(Bucket='ceph-bucket')\n",
    "s3.put_object(Bucket='ceph-bucket',Key='object',Body='data')\n",
    "for key in s3.list_objects(Bucket='ceph-bucket')['Contents']:\n",
    "    print(key['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running an application you can either establish a Spark session locally in the notebook pod, or point it to a remote Spark cluster. Oshinko is a collection of components from the radanalyticsio community that aid in the provisioning and scaling of Spark clusters for intelligent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[3]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", os.environ['RGW_API_ENDPOINT'])\n",
    "hadoopConf.set(\"fs.s3a.access.key\", os.environ['ACCESSKEY'])\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", os.environ['SECRETKEY'])\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "spark.range(100, numPartitions=100).rdd.map(lambda x: socket.gethostname()).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = spark.read.text(\"s3a://ceph-bucket/object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a Hybrid Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of Hadoop 2.8, S3A supports per bucket configuration. This is very powerful. It allows us to have a distinct S3A configuration, with a different endpoint and different set of credentials. With this I can use a single Spark context to read a parquet file from a bucket in the public cloud (Amazon S3) into a data frame, then turn around and write that dataframe as a parquet file into a bucket that exists in the Ceph Nano service running in Minishift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Public to Private ETL__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply read tab separated data from a bucket in Amazon S3 and write it back out to a bucket in our Ceph Nano service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"s3a://bd-dist/trip_report.tsv\",sep=\"\\t\").write.csv(\"s3a://ceph-bucket/trip_report.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all JSON files from a bucket prefix (pseudo directory) in Amazon S3 and write them back out to a bucket in our Ceph nano service with the same bucket prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.read.option(\"multiline\", True).option(\"mode\", \"PERMISSIVE\").json(\"s3a://bd-dist/kube-metrics\").repartition(76).write.option(\"compression\", \"bzip2\").json(\"s3a://ceph-bucket/kube-metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Prometheus data set from Ceph Nano into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFile = spark.read.json(\"s3a://ceph-bucket/kube-metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import statistics libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Display schema of files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Display schema:')\n",
    "jsonFile.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Query the JSON data using filters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the created SchemaRDD as a temporary table.\n",
    "jsonFile.registerTempTable(\"kubelet_docker_operations_latency_microseconds\")\n",
    "\n",
    "#Filter the results into a data frame\n",
    "data = spark.sql(\"SELECT values, metric.operation_type FROM kubelet_docker_operations_latency_microseconds WHERE metric.quantile='0.9' AND metric.hostname='free-stg-master-03fb6'\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_pd = data.toPandas()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "OP_TYPE = 'list_images'\n",
    "\n",
    "df2 = pd.DataFrame(columns = ['utc_timestamp','value', 'operation_type'])\n",
    "#df2 ='\n",
    "for op in set(data_pd['operation_type']):\n",
    "    dict_raw = data_pd[data_pd['operation_type'] == op]['values']\n",
    "    list_raw = []\n",
    "    for key in dict_raw.keys():\n",
    "        list_raw.extend(dict_raw[key])\n",
    "    temp_frame = pd.DataFrame(list_raw, columns = ['utc_timestamp','value'])\n",
    "    temp_frame['operation_type'] = op\n",
    "    \n",
    "    df2 = df2.append(temp_frame)\n",
    "\n",
    "\n",
    "df2 = df2[df2['value'] != 'NaN']\n",
    "\n",
    "df2['value'] = df2['value'].apply(lambda a: int(a))\n",
    "\n",
    "df2['timestamp'] = df2['utc_timestamp'].apply(lambda a : datetime.fromtimestamp(int(a)))\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Objective - Verify Above Alerts __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store timestamp with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index(inplace =True)\n",
    "\n",
    "del df2['index']\n",
    "\n",
    "df2['operation_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segregate the values by operation type in separate variables as Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_op_frame(op_type):\n",
    "    temp = df2[df2.operation_type == op_type]\n",
    "    temp = temp.sort_values(by='timestamp')\n",
    "    return temp\n",
    "\n",
    "operation_type_value = {}\n",
    "for temp in list(df2.operation_type.unique()):\n",
    "    operation_type_value[temp] = get_filtered_op_frame(temp)['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Descriptive Stats__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It refers to the portion of statistics dedicated to summarizing a total population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Mean_\n",
    "\n",
    "Arithmetic average of a range of values or quantities, computed by dividing the total of all values by the number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in operation_type_value.keys():\n",
    "    print(\"Mean of: \",temp, \" - \", np.mean(operation_type_value[temp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Variance_\n",
    "\n",
    "In the same way that the mean is used to describe the central tendency, variance is intended to describe the spread. The xi – μ is called the “deviation from the mean”, making the variance the squared deviation multiplied by 1 over the number of samples. This is why the square root of the variance, σ, is called the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in operation_type_value.keys():\n",
    "    print(\"Variance of: \",temp, \" - \", np.var(operation_type_value[temp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Standard Deviation_\n",
    "\n",
    "Standard deviation (SD, also represented by the Greek letter sigma σ or the Latin letter s) is a measure that is used to quantify the amount of variation or dispersion of a set of data values.[1] A low standard deviation indicates that the data points tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in operation_type_value.keys():\n",
    "    print(\"Standard Deviation of: \",temp, \" - \", np.std(operation_type_value[temp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Median_\n",
    "\n",
    "Denotes value or quantity lying at the midpoint of a frequency distribution of observed values or quantities, such that there is an equal probability of falling above or below it. Simply put, it is the middle value in the list of numbers. The median is a better choice when the indicator can be affected by some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in operation_type_value.keys():\n",
    "    print(\"Median of: \",temp, \" - \", np.median(operation_type_value[temp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Histogram__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common representation of a distribution is a histogram, which is a graph that shows the frequency or probability of each value. Plots will be generated by operation type\n",
    "\n",
    "We will use Seaborn module for this. __Kernel Density Estimation__ * will be added for smoothing.\n",
    "\n",
    "* In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample.\n",
    "* The kernel density estimate may be less familiar, but it can be a useful tool for plotting the shape of a distribution. Like the histogram, the KDE plots encodes the density of observations on one axis with height along the other axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(color_codes = True)\n",
    "\n",
    "for temp in operation_type_value.keys():\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,12))\n",
    "    sns.distplot(get_filtered_op_frame(temp)['value'], kde=True, ax=ax[0], axlabel= temp)\n",
    "    sns.distplot(np.log(get_filtered_op_frame(temp)['value']), kde=True, ax=ax[1], axlabel = \"Log transformed \"+ temp)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Understanding__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are all log normals, cause value will always be greater than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Box-Whisker__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.\n",
    "\n",
    "Log normalisation is required because, for different operations, values seems to be in very different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whisker =  df2\n",
    "df_whisker['log_transformed_value'] = np.log(df2['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whisker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "ax = sns.boxplot(x=\"operation_type\", y=\"log_transformed_value\", hue=\"operation_type\", data=df_whisker)  # RUN PLOT   \n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding trend in time series, if there any__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trend means, if over time values have increasing or decreasing pattern. In this example we see that there is a trend of a slow and steady increase followed by a sharp drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "operation_type_value.keys()\n",
    "\n",
    "for temp in operation_type_value.keys():\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,12))\n",
    "    temp_frame = get_filtered_op_frame(temp)\n",
    "    temp_frame = temp_frame.set_index(temp_frame.timestamp)\n",
    "    temp_frame = temp_frame[['log_transformed_value']]\n",
    "    temp_frame.plot(figsize=(15,12),title=temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
