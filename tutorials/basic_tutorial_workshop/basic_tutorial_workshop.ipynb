{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Data Hub - Basic Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intent of this notebook is to provide examples of how data scientists can use Open Data Hub with object storage, and more specifically, Ceph object storage, much in the same way they are accoustomed to interacting with Amazon Simple Storage Service (S3) for data science work.\n",
    "\n",
    "*Table of Contents:*\n",
    "1. Working with Boto\n",
    "2. Working with a Tensorflow Neural Network example\n",
    "3. Working with Spark and machine learning libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Boto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boto is an integrated interface to current and future infrastructural services offered by Amazon Web Services. It provides interfaces into Amazon S3 and Ceph Object Storage, two object stores often used for data lakes, along with many other services. For lightweight analysis of data using python tools like numpy or pandas, it is handy to interact with data stored in object storage using pure python. This is where Boto shines. Some notebooks from [Open Data Hub](https://radanalytics.io) may not include Boto, but you can install it from the comfort of a notebook using the conda install command below. If you find yourself using Boto frequently, it might be worth modifying [base-notebook](https://github.com/radanalyticsio/base-notebook) and building a custom notebook image that includes Boto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll use environment variables passed into the notebook from OpenShift for access to the Ceph Object Storage and Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3','us-east-1', endpoint_url= os.environ['S3_ENDPOINT_URL'],\n",
    "                       aws_access_key_id = os.environ['S3_ACCESS_KEY'],\n",
    "                       aws_secret_access_key = os.environ['S3_SECRET_KEY'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bucket, uploading an example object with the 'put' statement, and listing the bucket contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3.create_bucket(Bucket=os.environ['ATTENDEE_ID'])\n",
    "s3.put_object(Bucket=os.environ['ATTENDEE_ID'],Key='object',Body='data')\n",
    "for key in s3.list_objects(Bucket=os.environ['ATTENDEE_ID'])['Contents']:\n",
    "    print(key['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a Tensorflow Neural Network example\n",
    "\n",
    "Before we do anything else with Ceph and data, let's run a Tensorflow example.  We'll start by installing several machine learning libraries that we will need for our machine learning example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras==2.1.2 scikit-learn tensorflow matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a 2-hidden layers fully connected neural network (a.k.a multilayer perceptron) with TensorFlow.\n",
    "\n",
    "This example is using some of TensorFlow higher-level wrappers (tf.estimators, tf.layers, tf.metrics, ...), you can check 'neural_network_raw' example for a raw, and more detailed TensorFlow implementation.\n",
    "\n",
    "- Author: Aymeric Damien\n",
    "- Project: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Overview\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"nn\" style=\"width: 400px;\"/>\n",
    "\n",
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "def neural_net(x_dict):\n",
    "    # TF Estimator input is a dict, in case of multiple inputs\n",
    "    x = x_dict['images']\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.layers.dense(x, n_hidden_1)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.layers.dense(layer_1, n_hidden_2)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.layers.dense(layer_2, num_classes)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    logits = neural_net(features)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict single images\n",
    "n_images = 4\n",
    "# Get images from test set\n",
    "test_images = mnist.test.images[:n_images]\n",
    "# Prepare the input data\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': test_images}, shuffle=False)\n",
    "# Use the model to predict the images class\n",
    "preds = list(model.predict(input_fn))\n",
    "\n",
    "# Display\n",
    "for i in range(n_images):\n",
    "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
    "    plt.show()\n",
    "    print(\"Model prediction:\", preds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Spark and machine learning libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running an application you can either establish a Spark session locally in the notebook pod, or point it to a remote Spark cluster running in OpenShift or somewhere else externally accessible.  For this tutorial, each data scientist is given their own Spark environment running in OpenShift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by installing several machine learning libraries that we will need for our machine learning example.  In this example, we will be creating a model for detecting sentiment in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "#Add the necessary Hadoop and AWS jars to access Ceph from Spark\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk:1.7.4 pyspark-shell'\n",
    "\n",
    "spark = SparkSession.builder.master('local[3]').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the parameters for connecting Spark to Ceph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", os.environ['S3_ENDPOINT_URL'])\n",
    "hadoopConf.set(\"fs.s3a.access.key\", os.environ['S3_ACCESS_KEY'])\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", os.environ['S3_SECRET_KEY'])\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run a basic Spark command to test out the connection to Spark.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "spark.range(5, numPartitions=5).rdd.map(lambda x: socket.gethostname()).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the contents of the file uploaded to your Ceph bucket a few steps earlier using Spark and display it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = spark.read.text(\"s3a://\" + os.environ['ATTENDEE_ID'] + \"/object\")\n",
    "df0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload a sample data set to use for training the sentiment analysis model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the wget library to download data from online\n",
    "!pip install wget\n",
    "\n",
    "import wget\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3','us-east-1', endpoint_url= os.environ['S3_ENDPOINT_URL'],\n",
    "                       aws_access_key_id = os.environ['S3_ACCESS_KEY'],\n",
    "                       aws_secret_access_key = os.environ['S3_SECRET_KEY'])\n",
    "\n",
    "#upload the text file to Ceph\n",
    "url = \"https://gitlab.com/opendatahub/opendatahub-operator/raw/master/tutorials/basic_workshop_tutorial/sample_text_data.tsv?inline=false\"\n",
    "filename = wget.download(url=url, out='sample_text_data.tsv')\n",
    "s3.upload_file(filename, os.environ['ATTENDEE_ID'], \"sample_text_data.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Access the data using Spark__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feedbackFile = spark.read.csv(\"s3a://\" + os.environ['ATTENDEE_ID'] + \"/sample_text_data.tsv\",sep=\"\\t\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert the data to a Pandas data frame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = feedbackFile.toPandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Types of trip outcomes by field representatives__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(sum(map(ord, \"categorical\")))\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "outcome_dict = {'Successful':0,'Partial Success':1,'Unsuccessful':2 }\n",
    "\n",
    "df_vis = df[['Your Name', 'Outcome']]\n",
    "df_vis['outcome_numeric'] = df_vis['Outcome'].apply(lambda a:outcome_dict[a])\n",
    "\n",
    "\n",
    "\n",
    "outcome_cross_table = pd.crosstab(index=df_vis[\"Your Name\"], \n",
    "                          columns=df_vis[\"Outcome\"])\n",
    "\n",
    "\n",
    "outcome_cross_table.plot(kind=\"bar\", \n",
    "                 figsize=(16,12),\n",
    "                 stacked=True,fontsize=12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Types of outcomes by event type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type_cross_table = pd.crosstab(index=df[\"Primary Audience Engaged\"], \n",
    "                          columns=df[\"Outcome\"])\n",
    "\n",
    "event_type_cross_table.plot(kind=\"bar\", \n",
    "                 figsize=(16,12),\n",
    "                 stacked=True,fontsize=12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now convert \"Highlights\" data to prepare for training the sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Highlights'] = df['Highlights'].astype(str)\n",
    "\n",
    "df[['Highlights','Outcome']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcome = df[['Highlights','Outcome']]\n",
    "\n",
    "grouped_highlights = pd.DataFrame(df_outcome.groupby('Outcome')['Highlights'].apply(lambda x: \"%s\" % ' '.join(x)))\n",
    "\n",
    "grouped_highlights['Outcome'] = list(grouped_highlights.index.get_values())\n",
    "grouped_highlights.reset_index(drop=True, inplace=True)\n",
    "\n",
    "grouped_highlights['Highlights'] = grouped_highlights['Highlights'].astype(str)\n",
    "\n",
    "df['Highlights'] = df['Highlights'].apply(lambda a: a.lower())\n",
    "\n",
    "df_success = df[df['Outcome'] == 'Successful']\n",
    "df_unsuccess = df[df['Outcome'] == 'Unsuccessful']\n",
    "df_part_success = df[df['Outcome'] == 'Partial Success']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import additional Machine Learning libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Separating train and test data. Taking successful and unsuccessful separately__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failure = df_part_success.append(df_unsuccess, ignore_index= True)\n",
    "\n",
    "df_failure['Outcome'] = 'Unsuccessful'\n",
    "\n",
    "test_hold_out = 0.1\n",
    "\n",
    "#### Success\n",
    "\n",
    "train = df_success[ : -int(test_hold_out * len(df_success))]\n",
    "test = df_success[-int(test_hold_out * len(df_success)) : ]\n",
    "\n",
    "#### Failure\n",
    "\n",
    "train = train.append(df_failure[ : -int(test_hold_out * len(df_failure))])\n",
    "test = test.append(df_failure[-int(test_hold_out * len(df_failure)) : ])\n",
    "\n",
    "\n",
    "train = train.sample(frac = 1)\n",
    "train['type'] = \"Train\"\n",
    "test['type'] = \"Test\"\n",
    "\n",
    "train = train.append(test)\n",
    "\n",
    "train.reset_index(drop=True,inplace=True)\n",
    "\n",
    "Y = pd.get_dummies(train['Outcome']).values\n",
    "\n",
    "test_index_list = list(train[train['type'] == 'Test'].index)\n",
    "\n",
    "test_index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the HIGHLIGHTS field for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__max_features__ = Vocabulary size, its a hyper parameter\n",
    "\n",
    "*Tokenizer creates vectors from text, mainly works like a dictionary id in total vocabulary, returns list of integers, where every integer acts like an index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 10000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(train['Highlights'].values)\n",
    "X_highlights = tokenizer.texts_to_sequences(train['Highlights'].values)\n",
    "X_highlights = pad_sequences(X_highlights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating the network layer by layer__\n",
    "\n",
    "First layer is word embedding layer, second layer is LSTM based RNN, and third layer is Softmax activation layer, due to categorical outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X_highlights.shape[1], dropout=0.05))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.1, dropout_W=0.1))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Separating train and test data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_highlights_train = X_highlights[0:test_index_list[0]]\n",
    "Y_highlights_train = Y[0:test_index_list[0]]\n",
    "\n",
    "X_highlights_test = X_highlights[test_index_list[0]:]\n",
    "Y_highlights_test = Y[test_index_list[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Running the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "model.fit(X_highlights_train, Y_highlights_train, epochs = 10, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Printing test data accuracy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,accuracy = model.evaluate(X_highlights_test, Y_highlights_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"accuracy: %.2f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the model, tokenizer and feature dimension in Ceph for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "feature_dimension = X_highlights_train.shape[1]\n",
    "with open('./feature_dimension.pickle', 'wb') as handle:\n",
    "    pickle.dump(feature_dimension, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#Create S3 session for writing manifest file\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = os.environ['S3_ACCESS_KEY'],\n",
    "    aws_secret_access_key = os.environ['S3_SECRET_KEY']\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3', endpoint_url=os.environ['S3_ENDPOINT_URL'], verify=False)\n",
    "\n",
    "# Upload the model to S3\n",
    "s3.meta.client.upload_file('./model', os.environ['ATTENDEE_ID'], 'models/trip_report_model')\n",
    "\n",
    "# Upload the tokenizer to S3\n",
    "s3.meta.client.upload_file('./tokenizer.pickle', os.environ['ATTENDEE_ID'], 'models/trip_report_tokenizer.pickle')\n",
    "\n",
    "# Upload the feature dimension to S3\n",
    "s3.meta.client.upload_file('./feature_dimension.pickle', os.environ['ATTENDEE_ID'], 'models/trip_report_feature_dimension.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been saved to Ceph as binary objects and can be viewed or used at a later time.  You should see three model files from the above step now stored in Ceph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3','us-east-1', endpoint_url= os.environ['S3_ENDPOINT_URL'],\n",
    "                       aws_access_key_id = os.environ['S3_ACCESS_KEY'],\n",
    "                       aws_secret_access_key = os.environ['S3_SECRET_KEY'])\n",
    "\n",
    "for key in s3.list_objects(Bucket=os.environ['ATTENDEE_ID'], Prefix='models/')['Contents']:\n",
    "    print(key['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for participating in the Open Data Hub workshop!  For more information on the project or how to contribute check out [OpenDataHub.io](https://opendatahub.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
